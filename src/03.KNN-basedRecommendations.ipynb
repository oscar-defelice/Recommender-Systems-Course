{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Recommender Systems\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"721\" alt=\"cover-image\" src=\"https://user-images.githubusercontent.com/49638680/204351915-373011d3-75ac-4e21-a6df-99cd1c552f2c.png\">\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "# KNN Recommendations\n",
    "\n",
    "In this lecture, we are going to see a first example of non-personalised recommendation system. \n",
    "We have already seen that non-personalised recommendations are exploiting the distance between items, recommending the items that are \"close\" to the previously liked ones.\n",
    "\n",
    "The issue is that we never defined what \"_close_\" or \"_far_\" means in this context.\n",
    "\n",
    "The main idea inspiring the algorithm is defining vectors of items (in our case, movies) in order to _learn_ which movies are _closer_ to the last appreciated one. This approach goes under tha name of _item-based collaborative filtering_. We are going to build later in the course a first example of personalised recommendations by a _user-based collaborative filtering_.\n",
    "\n",
    "Now, let's briefly review the $k$-nn algorithm before applying it to building recommendations.\n",
    "\n",
    "## KNN algorithm\n",
    "\n",
    "$k$-nn is probably the simplest machine learning algorithm, from a certain perspective. \n",
    "Indeed, it is non-parametric, meaning that the algorithm does not have to estimate any parameter in order to _learn_.\n",
    "It is based on _distance_, trying to estimate the target value of new points from the first $k$ neighbours points labels or values.\n",
    "\n",
    "### Recall: Machine Learning\n",
    "\n",
    "The general idea of machine learning is to get a model to learn trends from historical data on any topic and be able to reproduce those trends on comparable data in the future. Here is a diagram outlining the basic machine learning process:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"699\" alt=\"image\" src=\"https://files.realpython.com/media/knn_01_MLgeneral_wide.74e5e2dc1094.png\">\n",
    "</p>\n",
    "\n",
    "This graph is a visual representation of a machine learning model that is fitted onto historical data. On the left are the original observations with three variables: height, width, and shape. The shapes are stars, crosses, and triangles.\n",
    "\n",
    "The shapes are located in different areas of the graph. On the right, you see how those original observations have been translated to a decision rule. For a new observation, you need to know the width and the height to determine in which square it falls. The square in which it falls, in turn, defines which shape it is most likely to have.\n",
    "\n",
    "Many different models could be used for this task. \n",
    "\n",
    "A **model** is a mathematical formula that can be used to describe data points. \n",
    "One example is the linear model, which uses a linear function defined by the formula \n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x\\, .$$\n",
    "\n",
    "**Fitting** a model means finding the optimal values for the fixed parameters using some algorithm. \n",
    "In the linear model, the parameters are $\\beta_0$ and $\\beta_1$. \n",
    "Luckily, you wonâ€™t have to invent such estimation algorithms to get started. \n",
    "Theyâ€™ve already been discovered by great mathematicians.\n",
    "\n",
    "Once the model is estimated, it becomes a mathematical formula in which you can fill in values for your independent variables to make predictions for your target variable. From a high-level perspective, thatâ€™s all that happens!\n",
    "\n",
    "### Distinguishing Features of $k$-NN\n",
    "\n",
    "Let's have a look at the noteworthy features of $k$-NN.\n",
    "\n",
    "#### kNN is a Supervised Machine Learning Algorithm\n",
    "\n",
    "The $k$-NN algorithm is a supervised machine learning model. \n",
    "That means it predicts a target variable using one or multiple independent variables.\n",
    "\n",
    "In particular, it expects to analyse data made as couples $(x, y)$ where $x$ are commonly known as _features_ while $y$ is said _target variable_.\n",
    "\n",
    "#### kNN is non-parametric\n",
    "\n",
    "As mentioned above, for knn there are no parameters to estimate.\n",
    "It is an algorithm purely based on distances, meaning that all _features_ count in the same way.\n",
    "\n",
    "### The algorithm in steps\n",
    "\n",
    "1. Define $k$\n",
    "2. Define a distance metric â€” _e.g._ Euclidean distance ($2$-norm distance).\n",
    "3. For a new data point, find the $k$ nearest training points.\n",
    "4. Here it depends whether we have a classification or a regression problem.\n",
    "    - For classification, we combine neighbour classes in some way â€” usually _voting_ â€” to get a predicted class.\n",
    "    - For regression we combine neighbour labels by their _average_ or _median_ to get the predicted value.\n",
    " \n",
    "Let's explore these steps in details.\n",
    "\n",
    "\n",
    "The kNN algorithm is a little bit atypical as compared to other machine learning algorithms. As you saw earlier, each machine learning model has its specific formula that needs to be estimated. The specificity of the k-Nearest Neighbours algorithm is that this formula is computed not at the moment of fitting but rather at the moment of prediction. This is not the case for most other models.\n",
    "\n",
    "When a new data point arrives, the kNN algorithm, as the name indicates, will start by finding the nearest neighbours of this new data point. Then it takes the values of those neighbours and uses them as a prediction for the new data point.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"699\" alt=\"image\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/03/knn3.png\">\n",
    "</p>\n",
    "\n",
    "As an intuitive example of why this works, think of your neighbours. Your neighbours are often relatively similar to you. They are probably in the same socioeconomic class as you. Maybe they have the same type of work as you, maybe their children go to the same school as yours, and so on. But for some tasks, this kind of approach is not as useful. For instance, it would not make any sense to look at your neighbourâ€™s favourite color to predict yours.\n",
    "\n",
    "Hence, the $k$NN algorithm is based on the assumption that you can predict the features of a data point based on the features of its neighbours. \n",
    "\n",
    "#### â€œNearestâ€ means we need a distance\n",
    "\n",
    "kNN algorithm is built on the concept of _distance_. This concept has a very precise definitions in mathematics.\n",
    "\n",
    "A _distance_ $\\delta$ is a function \n",
    "\n",
    "$$ \\delta : \\Omega \\times \\Omega \\longrightarrow \\mathbb{R} $$ \n",
    "\n",
    "such that it satisfies the following properties:\n",
    "\n",
    "1. A distance is always non-negative. $\\delta(x, y) \\geq 0\\, , \\forall x, y \\in \\Omega$\n",
    "2. Separation, $\\delta(x, y) = 0 \\Leftrightarrow x = y\\, , \\forall x, y \\in \\Omega$\n",
    "3. Symmetry, $\\delta(x, y) = \\delta(y, x)\\, , \\forall x, y \\in \\Omega$\n",
    "4. Triangular Inequality, $\\delta(x, z) \\leq \\delta(x, y) + \\delta(y, z) \\, , \\forall x, y, z \\in \\Omega$\n",
    "\n",
    "Examples of distances when $\\Omega$ is a real vector space are (among others) [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance), [Cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity), [Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry).\n",
    "\n",
    "##### Exercise\n",
    "\n",
    "> *Implement Euclidean, Cosine and Manhattan distances in Python making use of numpy.*\n",
    "\n",
    "---\n",
    "\n",
    "### The algorithm\n",
    "\n",
    "Let's have a look at the drawing here.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"881\" alt=\"image\" src=\"https://user-images.githubusercontent.com/49638680/159125703-a6f683d0-5a03-43e2-9ae5-293c86fe4eb7.png\">\n",
    "</p>\n",
    "\n",
    "Roughly speaking: we can look the nearest data points (in this case using Euclidean distance) to the green circle (new sample $x$) and make a prediction. \n",
    "So if we look at only three neighbours (where $k = 3$) we can say that it belongs to class $1$ and if we look at the $7$ nearest neighbours ($k = 7$) we can say it belongs to class $2$.\n",
    "\n",
    "### Find the $k$ Nearest Neighbours\n",
    "\n",
    "Now that you have a way to compute the distance from any point to any point, you can use this to find the nearest neighbours of a point on which you want to make a prediction.\n",
    "\n",
    "You need to find a number of neighbours, and that number is given by $k$. The minimum value of $k$ is of course $1$. This means using only one neighbour for the prediction. The maximum is the number of data points that you have. This means using all neighbours. The value of $k$ is something that the user defines, you will see a lot of these quantities from now on. These are called _hyperparameters_. \n",
    "Cross validation procedures and optimization tools can help you with this, as you will see in the next lectures.\n",
    "\n",
    "Now, to find the nearest neighbours with respect to a point $x$ in NumPy, we need to simply apply the right function to data. As you have seen, you need to define distances on the vectors of the independent variables. \n",
    "\n",
    "Once you have the array of distances, it is enough to sort it by the magnitude of distances and pick the first $k$ elements.\n",
    "\n",
    "### Combining $k$ Nearest Neighbours labels\n",
    "\n",
    "Now, to produce predictions we need to find a way to assign a _value_ $\\hat{y}$ to the new point $x$, based on the $k$-nearest neighbours we just found.\n",
    "\n",
    "#### Classification\n",
    "If we are in a classification problem, $y_i$ are discrete values, representing classes. One method to assign a class to the new point is a procedure called _voting_.\n",
    "\n",
    "##### Voting\n",
    "**Majority Voting**: After you take the $k$ nearest neighbors, you take a â€œvoteâ€ of those neighboursâ€™ classes. The new data point is classified with whatever the majority class of the neighbours is. If you are doing binary classification, it is recommended that you use an odd number of neighbors to avoid tied votes. However, in a multi-class problem, it is harder to avoid ties. A common solution to this is to decrease $k$ until the tie is broken.\n",
    "\n",
    "**Distance Weighting**: Instead of directly taking votes of the nearest neighbors, you weight each vote by the distance of that instance from the new data point. A common weighting method is\n",
    "$$\\hat{y} = \\dfrac{\\sum_i w_i y_i}{\\sum_i w_i}\\, ,$$\n",
    "where the weights $w_i := \\sum_i \\tfrac{1}{(x-x_i)^2}$. The new data point is added into the class with the largest added weight. Not only does this decrease the chances of ties, but it also reduces the effect of outliers.\n",
    "\n",
    "#### Regression\n",
    "If we are in a regression problem on the other hand, $y_i$ are continuous values. We can predict the new value $\\hat{y}$ combining $y_i$ of neighbours.\n",
    "\n",
    "**Median**: We take the median value out of the $k$-nearest neighbours.\n",
    "**Weighted average**: The weights are defined as above and we calculate $\\hat{y}$ as the weighted average of $y_i$.\n",
    "\n",
    "#### Bonus: Radius Neighbours\n",
    "\n",
    "This is the same idea as a $k$ nearest neighbour classifier, but instead of finding the $k$ nearest neighbours, you find all the neighbours within a given radius. Setting the radius requires some domain knowledge; if your points are closely packed together, you could want to use a smaller radius to avoid having nearly every point vote.\n",
    "\n",
    "### KNN for recommendations\n",
    "\n",
    "Now, we have to apply this nice algorithm to recommend movies to users.\n",
    "\n",
    "As said, $k$-nn does not make any assumptions about data distribution, it uses feature similarity. \n",
    "When $k$-nn makes inference about a movie, it will calculate the â€œdistanceâ€ between the target movie and every other movie in its database, then it ranks its distances and returns the top $k$ nearest neighbour movies as the most similar movie recommendations.\n",
    "\n",
    "#### The code ðŸ‘¨â€ðŸ’»\n",
    "\n",
    "Finally we are going to put our hands on some real code to build our $k$-nn based recommender system.\n",
    "\n",
    "First, let's import necessary libraries and data as in the previous lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from utils.data_utils import load_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 13)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from movielens dataset\n",
    "\n",
    "df_rating, df_rating_test, df_users, df_items, df_matrix, n_users, n_items = load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the size of our rating matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of rating matrix: (1650, 943)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of rating matrix: {df_matrix.values.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are high numbers (and we are using a small version of the database). We definitely do not want to feed our $k$-nn algorithm with a such a big matrix. Furthermore, the rating dataframe is plenty of nan's.\n",
    "\n",
    "Hence, for more efficient calculation and less memory footprint, we need to transform the values of the dataframe into a _scipy sparse matrix_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_movie_features = csr_matrix(df_matrix.fillna(0).values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this did not change the shape of the matrix. \n",
    "Indeed, our training data has a very high dimensionality. \n",
    "\n",
    "$k$-NNâ€™s performance will suffer from [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality) if it uses _Euclidean distance_ in its objective function. \n",
    "Roughly speaking, Euclidean distance is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector (target movieâ€™s features). \n",
    "Instead, we will use _cosine similarity_ for nearest neighbour search. \n",
    "Luckily, there is a ready-to-use implementation of cosine similarity in nearest neighbour, the one provided by scikit-learn, that we are about to use.\n",
    "\n",
    "There is also another popular approach to handle nearest neighbour search in high dimensional data, [locality sensitive hashing](https://en.wikipedia.org/wiki/Locality-sensitive_hashing), which we will not cover in this lecture.\n",
    "\n",
    "##### Exercise (hard)\n",
    "\n",
    "Try to implement locality sensitive hashing for knn. [This blog post](https://towardsdatascience.com/locality-sensitive-hashing-for-music-search-f2f1940ace23) might be useful.\n",
    "\n",
    "#### The model\n",
    "\n",
    "Let's define the model, it will be _trained_ over movie vectors and will be called to give recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = NearestNeighbors(\n",
    "    metric=\"cosine\", algorithm=\"brute\", n_neighbors=20, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to provide recommendations given the last well-rated movie, or the user favourite movie.\n",
    "\n",
    "In order to do so, we can define a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recommendations(fav_movie_id: int, n_recommendations: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to make top n movie recommendations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fav_movie: int,\n",
    "        id of user input movie\n",
    "\n",
    "    n_recommendations: int,\n",
    "        number of recommendations to provide\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The top recommendations collected in a dataframe.\n",
    "    \"\"\"\n",
    "    # fit the model\n",
    "    model_knn.fit(mat_movie_features)\n",
    "\n",
    "    # build favourite movie vector\n",
    "    fav_vec = df_matrix.fillna(0).loc[fav_movie_id].values.reshape(1, -1)\n",
    "\n",
    "    # inference\n",
    "    distances, indices = model_knn.kneighbors(\n",
    "        fav_vec, n_neighbors=n_recommendations + 1\n",
    "    )\n",
    "\n",
    "    ## Method 1: Sort list of raw idx of recommendations\n",
    "    # raw_recommends = \\\n",
    "    #        sorted(\n",
    "    #            list(\n",
    "    #                zip(\n",
    "    #                    indices.squeeze().tolist(),\n",
    "    #                    distances.squeeze().tolist()\n",
    "    #                )\n",
    "    #            ),\n",
    "    #            key=lambda x: x[1]\n",
    "    #        )[:0:-1]\n",
    "\n",
    "    # Method 2: Sort dataframe of raw idx of recommendations\n",
    "    df_res = df_items.iloc[indices[0]].copy()\n",
    "    df_res[\"distances\"] = distances[0]\n",
    "    df_res.sort_values(by=\"distances\")\n",
    "\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>VideoReleaseDate</th>\n",
       "      <th>Url</th>\n",
       "      <th>unknown</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Children</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>...</th>\n",
       "      <th>Film-Noir</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Romance</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "      <th>distances</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MovieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Four Rooms (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Four%20Rooms%...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>Nick of Time (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Nick%20of%20T...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.591842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Austin Powers: International Man of Mystery (1...</td>\n",
       "      <td>02-May-1997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Austin%20Powe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.609912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>Kingpin (1996)</td>\n",
       "      <td>12-Jul-1996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Kingpin%20(1996)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.611404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Clerks (1994)</td>\n",
       "      <td>01-Jan-1994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Clerks%20(1994)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.616329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>Faces (1968)</td>\n",
       "      <td>01-Jan-1968</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Faces%20(1968)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.626278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Ace Ventura: Pet Detective (1994)</td>\n",
       "      <td>01-Jan-1994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Ace%20Ventura...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.630763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>Fled (1996)</td>\n",
       "      <td>19-Jul-1996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Fled%20(1996)</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.641586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>Trainspotting (1996)</td>\n",
       "      <td>19-Jul-1996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/Title?Trainspotting+(1996)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.644772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Desperado (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Desperado%20(...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.648739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>Canadian Bacon (1994)</td>\n",
       "      <td>01-Jan-1994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Canadian%20Ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.654936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Title         Date  \\\n",
       "MovieId                                                                   \n",
       "3                                        Four Rooms (1995)  01-Jan-1995   \n",
       "761                                    Nick of Time (1995)  01-Jan-1995   \n",
       "249      Austin Powers: International Man of Mystery (1...  02-May-1997   \n",
       "410                                         Kingpin (1996)  12-Jul-1996   \n",
       "42                                           Clerks (1994)  01-Jan-1994   \n",
       "822                                           Faces (1968)  01-Jan-1968   \n",
       "67                       Ace Ventura: Pet Detective (1994)  01-Jan-1994   \n",
       "829                                            Fled (1996)  19-Jul-1996   \n",
       "475                                   Trainspotting (1996)  19-Jul-1996   \n",
       "33                                        Desperado (1995)  01-Jan-1995   \n",
       "719                                  Canadian Bacon (1994)  01-Jan-1994   \n",
       "\n",
       "         VideoReleaseDate                                                Url  \\\n",
       "MovieId                                                                        \n",
       "3                     NaN  http://us.imdb.com/M/title-exact?Four%20Rooms%...   \n",
       "761                   NaN  http://us.imdb.com/M/title-exact?Nick%20of%20T...   \n",
       "249                   NaN  http://us.imdb.com/M/title-exact?Austin%20Powe...   \n",
       "410                   NaN  http://us.imdb.com/M/title-exact?Kingpin%20(1996)   \n",
       "42                    NaN   http://us.imdb.com/M/title-exact?Clerks%20(1994)   \n",
       "822                   NaN    http://us.imdb.com/M/title-exact?Faces%20(1968)   \n",
       "67                    NaN  http://us.imdb.com/M/title-exact?Ace%20Ventura...   \n",
       "829                   NaN     http://us.imdb.com/M/title-exact?Fled%20(1996)   \n",
       "475                   NaN      http://us.imdb.com/Title?Trainspotting+(1996)   \n",
       "33                    NaN  http://us.imdb.com/M/title-exact?Desperado%20(...   \n",
       "719                   NaN  http://us.imdb.com/M/title-exact?Canadian%20Ba...   \n",
       "\n",
       "         unknown  Action  Adventure  Animation  Children  Comedy  ...  \\\n",
       "MovieId                                                           ...   \n",
       "3              0       0          0          0         0       0  ...   \n",
       "761            0       1          0          0         0       0  ...   \n",
       "249            0       0          0          0         0       1  ...   \n",
       "410            0       0          0          0         0       1  ...   \n",
       "42             0       0          0          0         0       1  ...   \n",
       "822            0       0          0          0         0       0  ...   \n",
       "67             0       0          0          0         0       1  ...   \n",
       "829            0       1          1          0         0       0  ...   \n",
       "475            0       0          0          0         0       0  ...   \n",
       "33             0       1          0          0         0       0  ...   \n",
       "719            0       0          0          0         0       1  ...   \n",
       "\n",
       "         Film-Noir  Horror  Musical  Mystery  Romance  Sci-Fi  Thriller  War  \\\n",
       "MovieId                                                                        \n",
       "3                0       0        0        0        0       0         1    0   \n",
       "761              0       0        0        0        0       0         1    0   \n",
       "249              0       0        0        0        0       0         0    0   \n",
       "410              0       0        0        0        0       0         0    0   \n",
       "42               0       0        0        0        0       0         0    0   \n",
       "822              0       0        0        0        0       0         0    0   \n",
       "67               0       0        0        0        0       0         0    0   \n",
       "829              0       0        0        0        0       0         0    0   \n",
       "475              0       0        0        0        0       0         0    0   \n",
       "33               0       0        0        0        1       0         1    0   \n",
       "719              0       0        0        0        0       0         0    1   \n",
       "\n",
       "         Western  distances  \n",
       "MovieId                      \n",
       "3              0   0.000000  \n",
       "761            0   0.591842  \n",
       "249            0   0.609912  \n",
       "410            0   0.611404  \n",
       "42             0   0.616329  \n",
       "822            0   0.626278  \n",
       "67             0   0.630763  \n",
       "829            0   0.641586  \n",
       "475            0   0.644772  \n",
       "33             0   0.648739  \n",
       "719            0   0.654936  \n",
       "\n",
       "[11 rows x 24 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_recommendations(fav_movie_id=3, n_recommendations=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercises\n",
    "\n",
    "1. Implement a full object that contains `make_recommendations` as a method. The object will be initialised with the data paths and will build the model, a method to associate the title to movie id and will provide recommendations with (a small edit of) the function defined above.\n",
    "2. Modify the algorithm of the previous point, such that you never recommend a movie the user has already rated.\n",
    "3. Use the object you built above to create a recommender system, that looks at your last rated movie (whose rating is above $4$) and recommends the _closest_ movies to that one.\n",
    "\n",
    "### Going further\n",
    "\n",
    "A further improvement of this system is to calculate the _center of appreciation_ for each user, by averaging the vectors of favourite movies per each user. This can be done for example by the average of all the movies a user gave a rating greater than $4$ for instance and find the $10$ closest movies to this point."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lectures",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "258d6f942e9abff99338ee3ea05bb7abc0fd3eb4d49f988f84979247168b5568"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
